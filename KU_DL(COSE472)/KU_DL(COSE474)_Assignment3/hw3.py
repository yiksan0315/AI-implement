import sys
import os
import matplotlib.pyplot as plt
import numpy as np


class nn_linear_layer:

    # linear layer.
    # randomly initialized by creating matrix W and bias b
    def __init__(self, input_size, output_size, std=1):
        self.W = np.random.normal(0, std, (output_size, input_size))
        self.b = np.random.normal(0, std, (output_size, 1))

    ######
    # Q1
    def forward(self, x):
        s = self.W @ x.T + self.b
        return s.T

    ######
    # Q2
    # returns three parameters
    def backprop(self, x, dLdy):
        """
        ∗ x. This is the input to this layer. Shape:(B, I)
        ∗ dLdy. This is the upstream gradient coming from the next layer.
        Shape:(B, O) Specifically, this parameter represents ∂L
        ∂y where y
        denotes the output of this layer. Thus backprop need to return
        the following gradient, which will be passed as the upstream gradient to the previous layer:
        we have two trainable parameters W and b.
        In order to perfom gradient descent, the following should be evaluated: for W, for b
        """

        n = x.shape[0]  # 20

        dLdW = np.einsum('ij,ik->ijk', dLdy, x).sum(axis=0)

        dLdb = dLdy.sum(axis=0)[None, :]

        dydx = np.array([self.W for _ in range(n)])
        dLdx = np.einsum('ij,ijk->ik', dLdy, dydx)

        return dLdW, dLdb, dLdx

    def update_weights(self, dLdW, dLdb):

        # parameter update
        self.W = self.W+dLdW
        self.b = self.b+dLdb


class nn_activation_layer:

    def __init__(self):
        pass

    ######
    # Q3
    def forward(self, x):
        sigmoid = 1/(1+np.exp(-x))
        return sigmoid

    ######
    # Q4
    def backprop(self, x, dLdy):
        """
        ∗ x. This is the input to this layer. Shape:(B, I)
        ∗ dLdy. This is the upstream gradient coming from the next layer.
        Shape:(B, I). Specifically, this parameter represents ∂L
        ∂y where y
        denotes the output of this layer. Thus backprop need to return
        the following gradient, which will be passed as the upstream gradient to the previous layer:

        return dLdx : has shape (B, I)

        x1: l1_out, dLdy: b_act_out
        """

        n = x.shape[0]  # 20
        class_num = x.shape[1]  # 4

        dydx = np.zeros((n, class_num, class_num))
        sigmoid = self.forward(x)

        for i in range(n):
            for j in range(class_num):
                # tensor with diagonal matrix
                dydx[i, j, j] = sigmoid[i, j] * (1-sigmoid[i, j])

        dLdx = np.zeros(x.shape)  # for better performance
        for i in range(n):
            for j in range(class_num):
                dLdx[i] = dLdy[i] * dydx[i].diagonal()

        return dLdx


class nn_softmax_layer:
    def __init__(self):
        pass

    ######
    # Q5
    def forward(self, x):
        exps = np.exp(x)
        softmax = exps / np.sum(exps, axis=1, keepdims=True)
        return softmax

    ######
    # Q6
    def backprop(self, x, dLdy):
        '''
        ∗ x. This is the input to this layer. Shape:(B,I)
        ∗ dLdy. This is the upstream gradient coming from the next layer.
        Shape:(B,I) Specifically, this parameter represents ∂L
        ∂y where y
        denotes the output of this layer. Thus backprop need to return
        the following gradient, which will be passed as the upstream gra-
        dient to the previous layer:

        return dLdx : has shape (B,I) : 20 x 2

        x: l2_out, dLdy: b_cent_out
        '''
        n = x.shape[0]  # 20
        class_num = x.shape[1]  # 2

        dydx = np.zeros((n, class_num, class_num))
        softmax = self.forward(x)

        for i in range(n):
            for j in range(class_num):
                for k in range(class_num):
                    if j == k:
                        dydx[i, j, k] = softmax[i, j] * (1-softmax[i, k])
                    else:
                        dydx[i, j, k] = -softmax[i, j] * softmax[i, k]

        dLdx = np.einsum('ij,ijk->ik', dLdy, dydx)

        return dLdx


class nn_cross_entropy_layer:
    def __init__(self):
        pass

    ######
    # Q7
    def forward(self, x, y):
        n = x.shape[0]  # 20

        logs = np.log(x)
        log_sum = 0

        for i in range(n):
            label = y[i][0]
            log_sum += logs[i, label]

        L = -log_sum/n

        return L

    ######
    # Q8
    def backprop(self, x, y):
        '''
            ∗ x. This is the input to this layer. Shape:(B,I) : softmax output
            ∗ y: ground truth labels. Shape:(B,1) : g.t.
            return dL/dx : has shape (B,I)
        '''

        n = x.shape[0]  # 20
        dLdx = np.zeros(x.shape)

        for i in range(n):
            dLdx[i, y[i]] = 1/x[i, y[i]]

        dLdx = -dLdx/n

        return dLdx


# number of data points for each of (0,0), (0,1), (1,0) and (1,1)
num_d = 5

# number of test runs
num_test = 40

# Q9. Hyperparameter setting
# learning rate (lr)and number of gradient descent steps (num_gd_step)
# This part is not graded (there is no definitive answer).
# You can set this hyperparameters through experiments.
lr = 0.1
num_gd_step = 4000

# dataset size
batch_size = 4*num_d

# number of classes is 2
num_class = 2

# variable to measure accuracy
accuracy = 0

# set this True if want to plot training data
# show_train_data = True
show_train_data = False

# set this True if want to plot loss over gradient descent iteration
show_loss = True

################
# create training data
################

m_d1 = (0, 0)
m_d2 = (1, 1)
m_d3 = (0, 1)
m_d4 = (1, 0)

sig = 0.05
s_d1 = sig ** 2 * np.eye(2)

d1 = np.random.multivariate_normal(m_d1, s_d1, num_d)
d2 = np.random.multivariate_normal(m_d2, s_d1, num_d)
d3 = np.random.multivariate_normal(m_d3, s_d1, num_d)
d4 = np.random.multivariate_normal(m_d4, s_d1, num_d)

# training data, and has shape (4*num_d,2)
x_train_d = np.vstack((d1, d2, d3, d4))
# training data lables, and has shape (4*num_d,1)
y_train_d = np.vstack((np.zeros((2 * num_d, 1), dtype='uint8'),
                      np.ones((2 * num_d, 1), dtype='uint8')))

if (show_train_data):
    plt.grid()
    plt.scatter(x_train_d[range(2 * num_d), 0],
                x_train_d[range(2 * num_d), 1], color='b', marker='o')
    plt.scatter(x_train_d[range(2 * num_d, 4 * num_d), 0], x_train_d[range(2 * num_d, 4 * num_d), 1], color='r',
                marker='x')
    plt.show()

################
# create layers
################

# hidden layer
# linear layer
layer1 = nn_linear_layer(input_size=2, output_size=4, )
# activation layer
act = nn_activation_layer()

# output layer
# linear
layer2 = nn_linear_layer(input_size=4, output_size=2, )
# softmax
smax = nn_softmax_layer()
# cross entropy
cent = nn_cross_entropy_layer()

# variable for plotting loss
loss_out = np.zeros((num_gd_step))

################
# do training
################

for i in range(num_gd_step):

    # fetch data
    x_train = x_train_d
    y_train = y_train_d

    ################
    # forward pass

    # hidden layer
    # linear
    l1_out = layer1.forward(x_train)
    # activation
    a1_out = act.forward(l1_out)

    # output layer
    # linear
    l2_out = layer2.forward(a1_out)
    # softmax
    smax_out = smax.forward(l2_out)

    # cross entropy loss
    loss_out[i] = cent.forward(smax_out, y_train)

    ################
    # perform backprop
    # output layer
    # cross entropy
    b_cent_out = cent.backprop(smax_out, y_train)
    # softmax
    b_nce_smax_out = smax.backprop(l2_out, b_cent_out)

    # linear
    b_dLdW_2, b_dLdb_2, b_dLdx_2 = layer2.backprop(
        x=a1_out, dLdy=b_nce_smax_out)

    # backprop, hidden layer
    # activation
    b_act_out = act.backprop(x=l1_out, dLdy=b_dLdx_2)
    # linear
    b_dLdW_1, b_dLdb_1, b_dLdx_1 = layer1.backprop(x=x_train, dLdy=b_act_out)

    ################
    # update weights: perform gradient descent
    layer2.update_weights(dLdW=-b_dLdW_2 * lr, dLdb=-b_dLdb_2.T * lr)
    layer1.update_weights(dLdW=-b_dLdW_1 * lr, dLdb=-b_dLdb_1.T * lr)

    if (i + 1) % 2000 == 0:
        print('gradient descent iteration:', i + 1)

# set show_loss to True to plot the loss over gradient descent iterations
if (show_loss):
    plt.figure(1)
    plt.grid()
    plt.plot(range(num_gd_step), loss_out)
    plt.xlabel('number of gradient descent steps')
    plt.ylabel('cross entropy loss')
    plt.show()

################
# training done
# now testing

num_test = 100

for j in range(num_test):

    predicted = np.ones((4,))

    # dispersion of test data
    sig_t = 1e-2

    # generate test data
    # generate 4 samples, each sample nearby (1,1), (0,0), (1,0), (0,1) respectively
    t11 = np.random.multivariate_normal((1, 1), sig_t**2*np.eye(2), 1)
    t00 = np.random.multivariate_normal((0, 0), sig_t**2*np.eye(2), 1)
    t10 = np.random.multivariate_normal((1, 0), sig_t**2*np.eye(2), 1)
    t01 = np.random.multivariate_normal((0, 1), sig_t**2*np.eye(2), 1)

    # predicting label for test sample nearby (1,1)
    l1_out = layer1.forward(t11)
    a1_out = act.forward(l1_out)
    l2_out = layer2.forward(a1_out)
    smax_out = smax.forward(l2_out)
    predicted[0] = np.argmax(smax_out)
    print('softmax out for (1,1)', smax_out,
          'predicted label:', int(predicted[0]))

    # predicting label for test sample nearby (0,0)
    l1_out = layer1.forward(t00)
    a1_out = act.forward(l1_out)
    l2_out = layer2.forward(a1_out)
    smax_out = smax.forward(l2_out)
    predicted[1] = np.argmax(smax_out)
    print('softmax out for (0,0)', smax_out,
          'predicted label:', int(predicted[1]))

    # predicting label for test sample nearby (1,0)
    l1_out = layer1.forward(t10)
    a1_out = act.forward(l1_out)
    l2_out = layer2.forward(a1_out)
    smax_out = smax.forward(l2_out)
    predicted[2] = np.argmax(smax_out)
    print('softmax out for (1,0)', smax_out,
          'predicted label:', int(predicted[2]))

    # predicting label for test sample nearby (0,1)
    l1_out = layer1.forward(t01)
    a1_out = act.forward(l1_out)
    l2_out = layer2.forward(a1_out)
    smax_out = smax.forward(l2_out)
    predicted[3] = np.argmax(smax_out)
    print('softmax out for (0,1)', smax_out,
          'predicted label:', int(predicted[3]))

    print('total predicted labels:', predicted.astype('uint8'))

    accuracy += (predicted[0] == 0) & (predicted[1] ==
                                       0) & (predicted[2] == 1) & (predicted[3] == 1)

    if (j + 1) % 10 == 0:
        print('test iteration:', j + 1)

print('accuracy:', accuracy / num_test * 100, '%')
